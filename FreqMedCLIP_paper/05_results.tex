\section{Analysis and Results}
\label{sec:results}

\subsection{Baseline Methods}
\label{subsec:baselines}

We compare FreqMedCLIP against four baselines that span the design space from single-stream to foundation-model approaches.

\paragraph{BiomedCLIP (Semantic-Only).}
Frozen BiomedCLIP ViT with a standard U-Net decoder and no frequency stream. This baseline establishes the performance ceiling for pure semantic processing and isolates the value added by frequency information.

\paragraph{Frequency-Only.}
A single ConvNeXt-Tiny frequency encoder on DWT inputs with a U-Net decoder and no semantic stream. This tests whether frequency features alone can support medical segmentation.

\paragraph{UNet-CLIP.}
Text-conditioned U-Net using CLIP embeddings for decoder conditioning without explicit frequency decomposition. This represents the standard text-guided single-stream design.

\paragraph{SAM-Med2D~\cite{cheng2024sammed2d}.}
The medical adaptation of SAM, representing the current strong foundation model baseline for 2D medical segmentation.

\subsection{Quantitative Results}
\label{subsec:quant}

Table~\ref{tab:main} presents results across the three datasets. FreqMedCLIP achieves the highest performance on all metrics and datasets.

\begin{table}[h]
\centering
\caption{Segmentation performance (Dice / IoU) on three benchmarks. All models use the same 70/20/10 data split. Results are averages over three random seeds. Bold: best per column.}
\label{tab:main}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
& \multicolumn{2}{c}{Brain Tumor} & \multicolumn{2}{c}{Breast} & \multicolumn{2}{c}{Lung Nodule} & \multicolumn{2}{c}{Average} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
Method & Dice & IoU & Dice & IoU & Dice & IoU & Dice & IoU \\
\midrule
BiomedCLIP (Sem.)    & $0.812$ & $0.686$ & $0.798$ & $0.664$ & $0.805$ & $0.673$ & $0.805$ & $0.674$ \\
Frequency-Only       & $0.731$ & $0.587$ & $0.715$ & $0.568$ & $0.728$ & $0.583$ & $0.725$ & $0.579$ \\
UNet-CLIP            & $0.834$ & $0.714$ & $0.821$ & $0.696$ & $0.827$ & $0.705$ & $0.827$ & $0.705$ \\
SAM-Med2D            & $0.851$ & $0.739$ & $0.839$ & $0.721$ & $0.844$ & $0.731$ & $0.845$ & $0.730$ \\
\textbf{FreqMedCLIP} & $\mathbf{0.873}$ & $\mathbf{0.779}$ & $\mathbf{0.861}$ & $\mathbf{0.759}$ & $\mathbf{0.866}$ & $\mathbf{0.767}$ & $\mathbf{0.867}$ & $\mathbf{0.768}$ \\
\bottomrule
\end{tabular}
\end{table}

The gains vary across datasets, and the pattern tells us something useful. FreqMedCLIP leads most strongly on brain tumour segmentation (Dice 0.873), where complex multi-region boundaries between necrotic core, oedema, and enhancing tumour demand precisely the complementary strengths of semantic context and frequency edge sensitivity. On breast mammography (Dice 0.861), dense tissue backgrounds create ambiguous boundaries that the semantic stream alone handles poorly (0.798). The frequency stream disambiguates fine-grained transitions that exhibit weak but consistent intensity edges in the Haar sub-bands, while language guidance (``breast cancer lesion'') suppresses false activations on benign dense tissue. Lung nodule segmentation (Dice 0.866) shows the narrowest gap over SAM-Med2D (+2.2\%). Nodules tend to have relatively clean circular boundaries where frequency-only methods already perform reasonably. Here the main benefit of our design is preventing false positives on vessels and bronchial walls via semantic gating.

The frequency-only baseline (0.725 Dice) confirms that frequency information alone is not enough: without semantic context, the encoder activates on all image edges indiscriminately. The semantic-only BiomedCLIP baseline (0.805 Dice) confirms the complementarity hypothesis. The 6.2\% absolute gap between single-stream semantic and our full dual-stream model justifies the modest computational overhead (55\% additional inference time, see Table~\ref{tab:efficiency}).

\subsection{Ablation Studies}
\label{subsec:ablation}

Table~\ref{tab:ablation} reports component-wise ablations on the average Dice metric. Each column shows the change relative to the full FreqMedCLIP model when the named component is removed.

\begin{table}[h]
\centering
\caption{Ablation results. Each column shows $\Delta$Dice when the named component is removed from the full model. All variants trained with the same protocol, reported over three seeds.}
\label{tab:ablation}
\begin{tabular}{@{}lcccccc@{}}
\toprule
 & Full Model & w/o FreqEnc & w/o FFBI & w/o LFFI & w/o Dual Dec & w/o TCA \\
\midrule
Avg Dice   & 0.867 & 0.825 & 0.837 & 0.846 & 0.852 & 0.861 \\
$\Delta$   & ---   & $-0.042$ & $-0.030$ & $-0.021$ & $-0.015$ & $-0.006$ \\
\bottomrule
\end{tabular}
\end{table}

The frequency encoder is the single most important component ($-4.2\%$ without it), confirming that explicit boundary information from DWT sub-bands provides signal the semantic stream can't recover on its own due to attention smoothing. Removing FFBI costs 3.0\%, showing that bidirectional cross-attention, which lets each stream selectively query the other, outperforms the na\"ive concatenation that removing FFBI reduces to. LFFI ($-2.1\%$) validates that language-guided FiLM modulation at multiple decoder scales is effective beyond what the visual encoder alone provides: the text signal guides which frequency components get upweighted at each spatial scale, preventing semantic drift as resolution increases. The dual-decoder ensemble ($-1.5\%$) and TCA adapters ($-0.6\%$) provide smaller but consistent improvements.

\subsection{Boundary Quality Analysis}
\label{subsec:boundary}

Table~\ref{tab:boundary} reports Hausdorff Distance and Boundary Dice, which are more sensitive to localisation precision than overlap metrics.

\begin{table}[h]
\centering
\caption{Boundary quality metrics. HD (mm): lower is better. Boundary Dice: higher is better.}
\label{tab:boundary}
\begin{tabular}{@{}lcc@{}}
\toprule
Method & Avg HD (mm) & Boundary Dice \\
\midrule
BiomedCLIP (Sem.) & $8.2$ & $0.742$ \\
Frequency-Only    & $3.1$ & $0.701$ \\
SAM-Med2D         & $6.1$ & $0.798$ \\
\textbf{FreqMedCLIP} & $\mathbf{2.1}$ & $\mathbf{0.831}$ \\
\bottomrule
\end{tabular}
\end{table}

FreqMedCLIP achieves 2.1\,mm HD, a $3.9\times$ improvement over the semantic-only BiomedCLIP baseline and $2.9\times$ over SAM-Med2D. The frequency-only baseline achieves 3.1\,mm HD (better boundary localisation than SAM-Med2D at 6.1\,mm), yet its overall Dice is the lowest at 0.725. This dissociation is telling: precise boundary localisation without semantic context produces correct edges for the wrong objects. FreqMedCLIP resolves this by coupling both pathways. The FFBI module lets the semantic stream suppress spurious frequency activations on non-target structures, while the frequency stream sharpens boundaries that the semantic stream would otherwise blur.

\subsection{Qualitative Results}
\label{subsec:qualitative}

Fig.~\ref{fig:qualitative} shows representative segmentation outputs for each dataset, and Fig.~\ref{fig:activations} visualises the per-stream activations after FFBI fusion.

\begin{figure}[htbp]
\centering
\begin{tabular}{ccc}
\fbox{\rule{0pt}{3cm}\rule{4.5cm}{0pt}} &
\fbox{\rule{0pt}{3cm}\rule{4.5cm}{0pt}} &
\fbox{\rule{0pt}{3cm}\rule{4.5cm}{0pt}} \\
Brain Tumor (BraTS) & Breast (CBIS-DDSM) & Lung Nodule (LUNA16) \\
\end{tabular}
\caption{\textbf{Qualitative segmentation results.} One representative inference result per dataset.
Ground truth boundary shown in green, FreqMedCLIP prediction in red, SAM-Med2D in blue.
\emph{[Placeholder: replace with actual inference outputs before submission.]}}
\label{fig:qualitative}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{tabular}{cccc}
\fbox{\rule{0pt}{3cm}\rule{3cm}{0pt}} &
\fbox{\rule{0pt}{3cm}\rule{3cm}{0pt}} &
\fbox{\rule{0pt}{3cm}\rule{3cm}{0pt}} &
\fbox{\rule{0pt}{3cm}\rule{3cm}{0pt}} \\
Input image & Semantic stream & Frequency stream & Fused output \\
\end{tabular}
\caption{\textbf{Per-stream activation maps.} Feature saliency after FFBI fusion.
Semantic activations (centre-left) capture coarse anatomical regions. Frequency activations (centre-right) highlight boundary structures.
The fused map (right) combines both, producing sharper, semantically constrained boundaries.
\emph{[Placeholder: replace with Grad-CAM or similar visualisations before submission.]}}
\label{fig:activations}
\end{figure}

\subsection{Computational Efficiency}
\label{subsec:efficiency}

\begin{table}[h]
\centering
\caption{Inference time and GPU memory on a single A100.}
\label{tab:efficiency}
\begin{tabular}{@{}lcc@{}}
\toprule
Method & Inference Time (ms) & Peak GPU Memory (GB) \\
\midrule
BiomedCLIP (Sem.) & 52  & 8.1 \\
SAM-Med2D              & 156 & 12.3 \\
\textbf{FreqMedCLIP}   & 81  & 10.7 \\
\bottomrule
\end{tabular}
\end{table}

FreqMedCLIP runs at 81\,ms per image, faster than SAM-Med2D by $1.9\times$ while achieving higher accuracy. The dual-stream overhead (29\,ms relative to BiomedCLIP alone) comes primarily from the ConvNeXt-Tiny encoder. FFBI and the decoders add negligible latency. The 10.7\,GB peak memory requirement fits within commonly available clinical workstation GPUs (A40, RTX~4090), making deployment practical.
