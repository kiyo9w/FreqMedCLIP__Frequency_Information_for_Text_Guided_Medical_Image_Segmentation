\section{Proposed Method}
\label{sec:method}

\subsection{Overview}
\label{subsec:overview}

We propose a dual-stream text-guided medical image segmentation framework that disentangles
(i) \emph{global semantic understanding} and (ii) \emph{frequency-driven boundary cues}, then couples them
through a bottleneck interaction module and a dual-decoder design.
Given an input image $\mathbf{I}\in\mathbb{R}^{H\times W\times C}$ and a text prompt $p$, the model predicts
a binary mask $\mathbf{M}\in[0,1]^{H\times W}$.
Fig.~\ref{fig:arch} provides an architectural overview.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Arch.png}
    \caption{\textbf{Architecture overview.}
    FreqMedCLIP consists of (1) a BiomedCLIP ViT semantic encoder with an FPN adapter (top stream),
    (2) a Haar DWT + ConvNeXt-Tiny frequency encoder (bottom stream),
    (3) a bottleneck bidirectional fusion block (FFBI),
    (4) two symmetric text-conditioned decoders, and (5) a learnable fusion head for final mask prediction.}
    \label{fig:arch}
\end{figure*}

\paragraph{Pipeline summary.}
The overall computation proceeds as:
\begin{equation}
\mathbf{S}_{1:4} = \mathrm{FPN}\big(\mathrm{SemEnc}(\mathbf{I},p)\big),\quad
\mathbf{F}_{\mathrm{stem},1:3} = \mathrm{FreqEnc}(\mathrm{DWT}(\mathbf{I})),
\quad [\hat{\mathbf{S}}_1,\hat{\mathbf{F}}_3] = \mathrm{FFBI}(\mathbf{S}_1,\mathbf{F}_3),
\end{equation}
\begin{equation}
\mathbf{L}_s=\mathrm{Dec}_s(\hat{\mathbf{S}}_1,\mathbf{S}_{2:4},p),\quad
\mathbf{L}_f=\mathrm{Dec}_f(\hat{\mathbf{F}}_3,\mathbf{F}_{\mathrm{stem},1:2},p),\quad
\mathbf{M}=\sigma\Big(\mathrm{Fuse}(\mathbf{L}_s,\mathbf{L}_f)\Big).
\end{equation}

\paragraph{Notation.}
The prompt $p$ is encoded into token embeddings $\mathbf{T}\in\mathbb{R}^{L\times d_t}$ and a pooled embedding
$\bar{\mathbf{t}}\in\mathbb{R}^{d_t}$. For a feature map $\mathbf{X}\in\mathbb{R}^{C\times h\times w}$,
$\mathrm{flat}(\mathbf{X})\in\mathbb{R}^{(hw)\times C}$ denotes spatial flattening into tokens.

\subsection{Input Processing}
\label{subsec:input}

We resize each image to $224\times224$ for the ViT-based semantic encoder (consistent with CLIP-style training),
and normalise intensities per modality (z-score for MRI, min-max for CT). The predicted mask is rescaled to the original
resolution using bilinear interpolation.

\subsection{Text Encoder}
\label{subsec:text}

We encode the prompt using the BiomedCLIP text encoder:
\begin{equation}
\mathbf{T} = \mathrm{TextEnc}(\mathrm{tokenize}(p)) \in \mathbb{R}^{L\times d_t},\qquad
\bar{\mathbf{t}}=\mathrm{Pool}(\mathbf{T})\in\mathbb{R}^{d_t},
\end{equation}
where $\mathrm{Pool}(\cdot)$ extracts the pooled end token. We use $\mathbf{T}$ for token-level cross-modal interaction
and $\bar{\mathbf{t}}$ for lightweight FiLM~\cite{perez2018film} modulation throughout the decoders.

\subsection{Semantic Stream: BiomedCLIP ViT with FPN Adapter}
\label{subsec:semantic}

\paragraph{Multi-layer ViT feature extraction.}
The BiomedCLIP vision transformer processes $\mathbf{I}$ and returns hidden states
$\{\mathbf{H}^{(\ell)}\}_{\ell=0}^{12}$. We extract four layers $\ell\in\{12,9,6,3\}$ (deep-to-shallow),
remove the class token, and reshape to spatial grids:
\begin{equation}
\mathbf{U}^{(\ell)} = \mathrm{reshape}\!\left(\mathbf{H}^{(\ell)}_{1:}\right)\in\mathbb{R}^{d_v\times 14\times 14},
\quad d_v=768.
\end{equation}

\paragraph{Text-Conditioned Adapters (TCA).}
To enable early text--vision interaction inside the encoder, we insert lightweight Text-Conditioned Adapters (TCA)
after selected ViT layers $\{3,6,9,12\}$. TCA performs token-level cross-attention from image tokens to text tokens:
\begin{equation}
\mathbf{X}^{(\ell)}_{\text{ca}} =
\mathbf{X}^{(\ell)} + \mathrm{CA}\big(\mathbf{X}^{(\ell)}\mathbf{W}_Q,\ \mathbf{T}\mathbf{W}_K,\ \mathbf{T}\mathbf{W}_V\big),
\end{equation}
followed by a small residual FFN:
\begin{equation}
\mathbf{X}^{(\ell)}_{\text{tca}} = \mathbf{X}^{(\ell)}_{\text{ca}} + \mathrm{FFN}\big(\mathrm{LN}(\mathbf{X}^{(\ell)}_{\text{ca}})\big).
\end{equation}
When full cross-attention is too costly, TCA can be replaced by FiLM-on-tokens:
$\mathbf{X}^{(\ell)}_{\text{tca}} = \boldsymbol{\gamma}^{(\ell)}\odot \mathbf{X}^{(\ell)} + \boldsymbol{\beta}^{(\ell)}$,
where $[\boldsymbol{\gamma}^{(\ell)},\boldsymbol{\beta}^{(\ell)}] = \mathrm{MLP}^{(\ell)}(\bar{\mathbf{t}})$.

\paragraph{FPN Adapter: semantic pyramid $\mathbf{S}_1\!\rightarrow\!\mathbf{S}_4$.}
Since ViT features are isotropic ($14\times14$), we construct a 4-level pyramid through progressive $2\times$
upsampling with lateral fusion:
\begin{equation}
\begin{aligned}
\mathbf{S}_1 &= \phi_{12}(\mathbf{U}^{(12)}) \in \mathbb{R}^{C_1\times 14\times 14}, \\
\mathbf{S}_2 &= \phi_{9}\!\Big(\mathrm{Up}_2(\mathbf{S}_1) \oplus \psi_{9}(\mathbf{U}^{(9)})\Big)
\in \mathbb{R}^{C_2\times 28\times 28}, \\
\mathbf{S}_3 &= \phi_{6}\!\Big(\mathrm{Up}_2(\mathbf{S}_2) \oplus \psi_{6}(\mathbf{U}^{(6)})\Big)
\in \mathbb{R}^{C_3\times 56\times 56}, \\
\mathbf{S}_4 &= \phi_{3}\!\Big(\mathrm{Up}_2(\mathbf{S}_3) \oplus \psi_{3}(\mathbf{U}^{(3)})\Big)
\in \mathbb{R}^{C_4\times 112\times 112}.
\end{aligned}
\end{equation}
$\psi_\ell$ are $1\times1$ channel-alignment projections; $\phi_\ell$ are $3\times3$ refinements.
$\mathbf{S}_1$ feeds FFBI; $\mathbf{S}_{2:4}$ serve as skip connections for the semantic decoder.

\subsection{Frequency Stream: Haar DWT + ConvNeXt-Tiny}
\label{subsec:frequency}

\paragraph{Haar DWT decomposition.}
We apply a one-level 2D discrete wavelet transform (Haar) to explicitly separate frequency bands:
\begin{equation}
[\mathbf{I}_{LL},\mathbf{I}_{LH},\mathbf{I}_{HL},\mathbf{I}_{HH}] = \mathrm{DWT}_{\mathrm{Haar}}(\mathbf{I}).
\end{equation}
$LL$ preserves coarse anatomy; $LH$/$HL$/$HH$ emphasise directional edges and fine detail.
We concatenate sub-bands channel-wise: $\mathbf{I}_{\mathrm{dwt}}=\mathrm{concat}(\mathbf{I}_{LL},\mathbf{I}_{LH},\mathbf{I}_{HL},\mathbf{I}_{HH})$.

\paragraph{ConvNeXt-Tiny frequency encoder pyramid.}
ConvNeXt-Tiny~\cite{liu2022convnet} serves as the frequency encoder backbone (first convolution modified to accept $\mathbf{I}_{\mathrm{dwt}}$):
\begin{equation}
[\mathbf{F}_{\mathrm{stem}},\mathbf{F}_1,\mathbf{F}_2,\mathbf{F}_3] = \mathrm{ConvNeXtTiny}(\mathbf{I}_{\mathrm{dwt}}),
\end{equation}
where
$\mathbf{F}_{\mathrm{stem}}\!\in\!\mathbb{R}^{C_s\times112\times112}$,
$\mathbf{F}_1\!\in\!\mathbb{R}^{C_1'\times56\times56}$,
$\mathbf{F}_2\!\in\!\mathbb{R}^{C_2'\times28\times28}$,
$\mathbf{F}_3\!\in\!\mathbb{R}^{C_3'\times14\times14}$.
$\mathbf{F}_3$ is fused with $\mathbf{S}_1$ in FFBI; $\mathbf{F}_{\mathrm{stem}}$, $\mathbf{F}_1$, $\mathbf{F}_2$
provide skip features for the frequency decoder.

\subsection{FFBI: Bottleneck Fusion Between Semantic and Frequency Streams}
\label{subsec:ffbi}

The Frequency--Feature Bridging Integration (FFBI) couples the two streams at the $14\times14$ bottleneck.
Both feature maps are flattened into token sequences and projected to a shared width $d$:
\begin{equation}
\tilde{\mathbf{Z}}_s=\mathrm{flat}(\mathbf{S}_1)\mathbf{W}_s,\quad
\tilde{\mathbf{Z}}_f=\mathrm{flat}(\mathbf{F}_3)\mathbf{W}_f,
\end{equation}
then fused via bidirectional cross-attention:
\begin{equation}
\hat{\mathbf{Z}}_s = \tilde{\mathbf{Z}}_s + \mathrm{CA}\!\left(\tilde{\mathbf{Z}}_s,\tilde{\mathbf{Z}}_f,\tilde{\mathbf{Z}}_f\right),\qquad
\hat{\mathbf{Z}}_f = \tilde{\mathbf{Z}}_f + \mathrm{CA}\!\left(\tilde{\mathbf{Z}}_f,\tilde{\mathbf{Z}}_s,\tilde{\mathbf{Z}}_s\right).
\end{equation}
This allows the semantic stream to query boundary cues from the frequency stream and vice versa, without
text bias at the fusion point. Optionally, a text gate is applied post-fusion:
\begin{equation}
[\boldsymbol{\gamma}_b,\boldsymbol{\beta}_b] = \mathrm{MLP}_b(\bar{\mathbf{t}}),\qquad
\hat{\mathbf{Z}}_s \leftarrow \boldsymbol{\gamma}_b\odot \hat{\mathbf{Z}}_s + \boldsymbol{\beta}_b,\quad
\hat{\mathbf{Z}}_f \leftarrow \boldsymbol{\gamma}_b\odot \hat{\mathbf{Z}}_f + \boldsymbol{\beta}_b.
\end{equation}
After reshaping, both fused maps are upsampled to $28\times28$ to initialise the two decoders:
\begin{equation}
\mathbf{D}_s^0=\mathrm{Up}_2(\hat{\mathbf{S}}_1),\qquad
\mathbf{D}_f^0=\mathrm{Up}_2(\hat{\mathbf{F}}_3).
\end{equation}

\subsection{Dual Decoders with Language-Guided Feature Fusion (LFFI)}
\label{subsec:decoders}

Two symmetric decoders produce features at $28\times28$, $56\times56$, and $112\times112$,
each conditioned on text via FiLM~\cite{perez2018film} at every stage (LFFI):
\begin{equation}
[\boldsymbol{\gamma},\boldsymbol{\beta}] = \mathrm{MLP}(\bar{\mathbf{t}}),\qquad
\mathrm{FiLM}(\mathbf{X},\bar{\mathbf{t}})=\boldsymbol{\gamma}\odot \mathbf{X} + \boldsymbol{\beta}.
\end{equation}
At each scale, a decoder stage merges the current feature map with a skip connection:
\begin{equation}
\mathbf{Y} = \mathrm{ConvBlock}\!\Big(\mathrm{concat}(\mathbf{X},\mathbf{S})\Big),\qquad
\mathbf{Y}' = \mathrm{FiLM}(\mathbf{Y},\bar{\mathbf{t}}),\qquad
\mathbf{X}_{\mathrm{next}} = \mathrm{Up}_2(\mathbf{Y}').
\end{equation}

\paragraph{Semantic decoder.}
Uses semantic FPN skips $\mathbf{S}_{2:4}$:
\begin{equation}
\mathbf{D}_s^1=\mathrm{Dec}_{28}(\mathbf{D}_s^0,\mathbf{S}_2),\quad
\mathbf{D}_s^2=\mathrm{Dec}_{56}(\mathbf{D}_s^1,\mathbf{S}_3),\quad
\mathbf{D}_s^3=\mathrm{Dec}_{112}(\mathbf{D}_s^2,\mathbf{S}_4).
\end{equation}

\paragraph{Frequency decoder.}
Uses frequency encoder skips $\{\mathbf{F}_{\mathrm{stem}},\mathbf{F}_1,\mathbf{F}_2\}$:
\begin{equation}
\mathbf{D}_f^1=\mathrm{Dec}_{28}(\mathbf{D}_f^0,\mathbf{F}_2),\quad
\mathbf{D}_f^2=\mathrm{Dec}_{56}(\mathbf{D}_f^1,\mathbf{F}_1),\quad
\mathbf{D}_f^3=\mathrm{Dec}_{112}(\mathbf{D}_f^2,\mathbf{F}_{\mathrm{stem}}).
\end{equation}

\subsection{Output Heads and Fusion}
\label{subsec:fusion}

Each decoder produces a single-channel logit map via a $1\times1$ convolution:
$\mathbf{L}_s=\mathrm{Head}_s(\mathbf{D}_s^3)$ and $\mathbf{L}_f=\mathrm{Head}_f(\mathbf{D}_f^3)$.
A learnable fusion head combines them:
\begin{equation}
\mathbf{L} = \mathrm{Head}_{\mathrm{fuse}}\!\big(\mathrm{concat}(\mathbf{L}_s,\mathbf{L}_f)\big),\qquad
\mathbf{M}=\sigma(\mathbf{L}).
\end{equation}
$\mathrm{Head}_{\mathrm{fuse}}$ is a small two-layer conv block that learns complementary branch weighting.

\subsection{Training Objective}
\label{subsec:loss}

The model is supervised with a combined Dice and binary cross-entropy loss:
\begin{equation}
\mathcal{L}_{\mathrm{seg}}=
\mathcal{L}_{Dice}(\mathbf{M},\mathbf{Y})+\lambda\,\mathcal{L}_{BCE}(\mathbf{M},\mathbf{Y}),
\end{equation}
where $\mathbf{Y}$ is the ground-truth binary mask. Optional deep supervision on branch logits further stabilises training:
\begin{equation}
\mathcal{L}=\mathcal{L}_{\mathrm{seg}}+
\alpha\,\mathcal{L}_{Dice}(\sigma(\mathbf{L}_s),\mathbf{Y})+
\beta\,\mathcal{L}_{Dice}(\sigma(\mathbf{L}_f),\mathbf{Y}),
\end{equation}
with small $\alpha,\beta$ to avoid over-constraining intermediate heads.

\paragraph{Trainable components.}
The FPN adapter, TCA adapters, FFBI, both decoders, and fusion head are trained end-to-end. The BiomedCLIP ViT
backbone is frozen by default; TCA adapter weights introduce fewer than 2\% additional parameters
relative to the total trainable parameter count.
