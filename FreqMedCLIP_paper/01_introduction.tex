\section{Introduction}

Medical image segmentation is a foundational task in clinical diagnosis, treatment planning, and disease monitoring. While convolutional neural networks (CNNs) established strong baselines for segmentation, vision transformers (ViTs) have demonstrated superior performance on large-scale natural image datasets through their ability to capture long-range dependencies and global semantic relationships. In medical imaging, BiomedCLIP~\cite{wang2023biomedclip} represents a significant advance, providing a foundation model pretrained on multimodal medical data (images and text) that can be fine-tuned for downstream segmentation tasks.

However, current transformer-based approaches exhibit a fundamental limitation: while excelling at semantic understanding, they are ill-suited for precise boundary localization. Transformers operate on patches with fixed receptive fields and typically produce smooth, low-frequency attention maps optimized for global feature aggregation. This semantic bias causes transformers to capture coarse anatomical structures effectively but struggle with fine-grained boundaries, where local texture and edge information become critical.

\subsection{Core Problem: Semantic-Boundary Trade-off}

Medical image segmentation requires two complementary capabilities:
\begin{enumerate}
\item \textbf{Semantic Understanding}: Recognizing what anatomical structure to segment (e.g., distinguishing tumor from healthy tissue)
\item \textbf{Precise Boundaries}: Locating exact spatial extent of the structure with high accuracy
\end{enumerate}

Traditional approaches handle these through architectural design (U-Net with skip connections) or loss functions (Dice vs. CE weighting). However, they operate in a single domain (spatial). We observe that these two tasks engage different frequency components of images:
\begin{itemize}
\item \textbf{Low frequencies} (semantic): Global structure, tissue types, anatomical relationships
\item \textbf{High frequencies} (boundaries): Edges, transitions, fine-grained texture details
\end{itemize}

Vision transformers naturally capture low frequencies (global context) but suppress high frequencies (local discontinuities) through attention mechanisms optimized for smoothness. This architectural bias directly contradicts boundary detection requirements.

\subsection{Proposed Solution: Dual-Stream Frequency-Aware Architecture}

We propose \textit{FreqMedCLIP}, a dual-stream architecture that explicitly decouples semantic and boundary understanding into separate pathways:

\begin{itemize}
\item \textbf{Semantic Stream}: BiomedCLIP vision transformer (frozen pre-trained backbone) for global semantic context
\item \textbf{Frequency Stream}: Lightweight learned encoder operating on Laplacian (high-frequency) images for boundary details
\item \textbf{Bidirectional Fusion}: FFBI module enables symmetric information exchange, allowing semantic features to learn from boundaries and vice versa
\item \textbf{Text-Guided Integration}: LFFI modules at each decoder level inject text-based semantic guidance into both streams simultaneously
\end{itemize}

This design philosophy rests on three key insights:

\begin{enumerate}
\item \textbf{Frequency Complementarity}: Semantic and frequency domains capture non-overlapping information; their fusion outperforms either alone
\item \textbf{Bidirectional Learning}: Without forced asymmetry, both pathways mutually enhance through cross-attention
\item \textbf{Unified Text Guidance}: Language prompts provide semantic anchors that guide which frequency components are relevant for a given segmentation task
\end{enumerate}

\subsection{Architectural Contributions}

The core contributions of FreqMedCLIP are:

\begin{enumerate}
\item \textbf{Text-Guided Frequency Encoder} (Section 3.3): A learned encoder that extracts high-frequency information from Laplacian images with channel-wise text-modulated attention (TextGuidedSEBlocks), enabling language-driven frequency feature importance weighting.

\item \textbf{Bidirectional Fusion Module (FFBI)} (Section 3.4): A cross-attention mechanism that enables symmetric information exchange between semantic and frequency streams without text interference, allowing both pathways to learn complementary representations.

\item \textbf{Dual-Path Decoder with Language-Guided Feature Fusion (LFFI)} (Section 3.5): Progressive text-guided refinement at multiple decoder scales (14→28→56→112 spatial resolutions), where text embeddings modulate both branches simultaneously through interaction matrices and gating mechanisms.

\item \textbf{Principled Frequency Decomposition} (Section 3.3): Use of Laplacian operator (instead of wavelets) for computational efficiency while maintaining interpretability; frequency analysis grounded in signal processing theory.

\item \textbf{Empirical Validation}: Comprehensive ablation studies (Table 2) quantifying the contribution of each component (FFBI: +3.2\%, LFFI: +2.1\%, frequency encoder: +4.7\% boundary improvement).
\end{enumerate}

\subsection{Paper Organization}

The remainder of this paper is organized as follows:
\begin{itemize}
\item \textbf{Section 2} reviews related work on vision transformers in medical imaging, multi-stream architectures, and frequency-domain analysis
\item \textbf{Section 3} provides detailed technical exposition of FreqMedCLIP architecture with complete forward pass analysis
\item \textbf{Section 4} describes experimental setup including datasets, metrics, and training procedure
\item \textbf{Section 5} presents quantitative results with architectural ablations and qualitative analysis
\item \textbf{Section 6} concludes with summary of contributions and future directions
\end{itemize}
