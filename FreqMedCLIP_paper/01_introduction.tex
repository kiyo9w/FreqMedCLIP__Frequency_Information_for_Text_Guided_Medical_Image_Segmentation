\section{Introduction}

Medical image segmentation underpins clinical workflows from tumor delineation to organ-at-risk contouring and lesion monitoring~\cite{litjens2017survey}. The task is dual in nature: a model must \emph{recognise} what it segments, establishing semantic correspondence between image regions and anatomical categories, and \emph{locate} the precise spatial extent of those structures, including fine boundary details that carry direct clinical weight. These two demands engage different parts of the image signal. Semantic recognition depends on long-range contextual cues and global intensity distributions. Boundary delineation requires sensitivity to local high-frequency transitions at tissue interfaces.

Convolutional architectures, led by U-Net~\cite{ronneberger2015u} and its descendants~\cite{isensee2021nnunet,oktay2018attention}, address this tension through hierarchical feature representations and skip connections. The inductive biases of convolution, however, limit capacity to model the global context needed to distinguish, for example, a necrotic tumor core from surrounding oedema in a glioma scan. Vision transformers (ViTs) replace local convolutions with self-attention over image patches~\cite{dosovitskiy2020image}, enabling long-range dependency modelling. Hybrid designs such as TransUNet~\cite{chen2024transunet} combine both paradigms. Foundation models built on ViT pretraining, including SAM~\cite{kirillov2023segment} and its medical adaptations~\cite{ma2024medsam}, as well as BiomedCLIP~\cite{zhang2025biomedclip}, have shown strong zero-shot and few-shot segmentation. Text-image alignment through CLIP-style training~\cite{radford2021learning}, extended by LViT~\cite{li2024lvit} and MedCLIP-SAM~\cite{koleilat2024medclip}, lets models condition segmentation on natural-language descriptions of the target structure.

Yet transformer-based architectures inherit a systematic weakness: their patch-level operation and attention smoothing suppress high-frequency image components that delineate anatomical boundaries. Attention maps across deep ViT layers tend toward low spatial frequency, capturing coarse layouts rather than boundary topology~\cite{park2022vision}. The result is segmentations that correctly identify the gross location of a structure but yield blurred or misaligned boundaries, inflating boundary-sensitive metrics such as Hausdorff distance. Frequency-domain methods have long been used for edge detection. Wavelet decomposition, Fourier analysis, and Laplacian operators all isolate high-frequency content~\cite{mallat1989wavelet}. They lack the semantic context, though, to distinguish meaningful anatomical boundaries from imaging noise or tissue texture.

This paper asks whether a single architecture can achieve strong semantic segmentation \emph{and} reliable boundary localisation by explicitly coupling the complementary strengths of semantic and frequency pathways. We answer yes with \textit{FreqMedCLIP}, a dual-stream architecture that pairs a frozen BiomedCLIP vision encoder with a learned frequency encoder operating on Haar wavelet sub-bands. The two streams interact at a shared bottleneck through bidirectional cross-attention (FFBI), and text embeddings derived from natural-language prompts modulate both decoder branches via FiLM conditioning at each spatial scale. The design rests on a signal-processing principle: low-frequency components encode global semantic structure while high-frequency components encode edges and fine detail. Fusing both under language guidance yields representations that are simultaneously semantically grounded and boundary-aware. On three diverse benchmarks (brain, breast, and lung segmentation), FreqMedCLIP achieves 0.867 average Dice coefficient and 2.1\,mm mean Hausdorff distance, compared with 0.845 Dice and 6.1\,mm HD for SAM-Med2D.

The principal contributions of this paper are:

\begin{enumerate}
    \item \textbf{A dual-stream frequency-semantic architecture.} We couple a pretrained biomedical vision transformer with a ConvNeXt-based frequency encoder fed on one-level Haar DWT sub-bands. The frequency front-end gives computationally efficient, theoretically grounded access to directional edge information that the ViT systematically suppresses. On three benchmarks, the dual-stream design improves average Dice by 6.2\% over the semantic-only baseline and reduces Hausdorff distance from 8.2\,mm to 2.1\,mm.

    \item \textbf{Bidirectional frequency-feature bridging integration (FFBI).} A bottleneck cross-attention module that enables symmetric information exchange between the semantic and frequency streams. Each stream selectively queries the other for complementary information, yielding representations that neither could produce alone. FFBI contributes 3.0\% absolute Dice improvement over concatenation-based fusion in ablation studies.

    \item \textbf{Language-guided feature fusion via FiLM modulation (LFFI).} Text embeddings from the BiomedCLIP language encoder modulate both decoder branches at every spatial resolution ($14\!\to\!28\!\to\!56\!\to\!112$), providing progressive semantic guidance that prevents semantic drift during upsampling. LFFI contributes 2.1\% Dice improvement and is most effective at intermediate decoder scales.
\end{enumerate}
