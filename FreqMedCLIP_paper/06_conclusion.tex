\section{Discussion}
\label{sec:discussion}

\paragraph{Interpreting the performance gains.}
The 6.2\% absolute Dice improvement over the semantic-only BiomedCLIP baseline is large given BiomedCLIP's strong pretraining on fifteen million image-text pairs. We attribute this to two complementary mechanisms. First, DWT sub-band inputs give the frequency encoder explicit access to high-frequency image components that the ViT's patch-level processing systematically suppresses through attention smoothing, a documented frequency bias~\cite{park2022vision}. Second, FFBI bidirectional cross-attention lets the semantic stream ground its boundary predictions in actual edge signals rather than learned prior distributions over anatomical shape, reducing the blurring artefacts common in transformer decoders. The improvement over SAM-Med2D (+2.2\% Dice, $2.9\times$ better HD) deserves examination because SAM-Med2D uses substantially more pretraining data. The gap is most pronounced in boundary metrics, confirming that FreqMedCLIP's advantage lies in localisation precision rather than semantic recognition.

\paragraph{Clinical relevance of boundary quality.}
The 2.1\,mm mean Hausdorff distance represents substantially tighter boundary localisation than the semantic-only baseline (8.2\,mm) and SAM-Med2D (6.1\,mm). In downstream workflows such as treatment planning and longitudinal lesion tracking, boundary errors propagate to volume estimates and margin decisions. The large HD reduction is therefore practically important even when Dice gains appear moderate. A formal clinical-impact analysis remains future work and would require task-specific prospective evaluation.

\paragraph{Frequency-semantic dissociation.}
The frequency-only baseline achieves lower HD (3.1\,mm) than SAM-Med2D (6.1\,mm) despite having dramatically lower Dice (0.725 vs.\ 0.845). This dissociation matters: SAM-Med2D produces segmentations that are semantically reasonable with smooth boundaries, while the frequency-only model draws sharp, localised edges that are geometrically precise when correct but frequently misattributed to non-target structures. This finding motivates the FFBI design choice directly. The semantic stream must suppress spurious frequency activations, not just concatenate with them.

\paragraph{Text guidance and its limits.}
LFFI provides a consistent 2.1\% Dice improvement across all three datasets. Analysis of the ablation variants reveals that text guidance works best at intermediate scales ($28\times28$, $56\times56$), where language conditioning helps the decoder commit to specific anatomical regions before refining fine details at $112\times112$. At the coarsest scale, the semantic stream already provides strong localisation. At the finest scale, spatial frequency features dominate. These observations align with the multi-scale language integration findings of LViT~\cite{li2024lvit}, though our FiLM-based implementation imposes less computational overhead than cross-attention conditioning.

\paragraph{Limitations.}
Three limitations deserve attention. \emph{Resolution constraints}: the $224\times224$ input resolution inherited from BiomedCLIP limits detection of nodules smaller than about 20 pixels. ViT patch tokens at $14\times14$ spatial resolution provide insufficient coverage for sub-centimetre structures. Hierarchical multi-scale inference or higher-resolution ViT backbones would address this. \emph{Diffuse boundaries}: while Haar DWT provides richer multi-directional sub-bands than a single Laplacian, diffuse infiltrative lesions (e.g., low-grade glioma) lack sharp intensity transitions and remain challenging for frequency-based boundary detection. Combining DWT with learned filterbanks may improve performance in these cases. \emph{3D extension}: the current design operates on 2D slices, discarding inter-slice context present in volumetric MRI and CT. Extending to 3D requires rethinking both the DWT decomposition (3D sub-bands) and the attention scalability of FFBI.

\section{Conclusion}
\label{sec:conclusion}

We have presented FreqMedCLIP, a dual-stream architecture for text-guided medical image segmentation that couples a frozen BiomedCLIP vision transformer with a learned frequency encoder processing Haar wavelet sub-bands. The two streams interact at a shared bottleneck via bidirectional cross-attention (FFBI), and language-guided FiLM conditioning (LFFI) modulates both decoder branches at every spatial resolution. Experiments on brain, breast, and lung benchmarks show consistent improvements over strong baselines: 0.867 average Dice (vs.\ 0.845 for SAM-Med2D), 2.1\,mm Hausdorff distance (vs.\ 6.1\,mm for SAM-Med2D), and 12\,M trainable parameters at 81\,ms inference. Ablation studies confirm that the frequency encoder provides the largest individual contribution ($+4.2\%$ Dice), followed by FFBI fusion ($+3.0\%$) and LFFI text guidance ($+2.1\%$), with all components jointly necessary for the full performance. Future work will explore adaptive frequency decomposition with learnable filterbanks, extension to 3D volumetric segmentation with axial DWT, and self-supervised pretraining that jointly learns semantic and frequency representations.
