\section{Related Work}

\paragraph{Vision Transformers and Foundation Models for Medical Segmentation.}
The Vision Transformer (ViT)~\cite{dosovitskiy2020image} showed that self-attention over patch sequences can match or exceed convolutional networks for image recognition. Park and Kim~\cite{park2022vision} found that ViTs learn low-frequency features more readily than CNNs, with attention maps exhibiting strong spatial smoothness, a property beneficial for semantic understanding but harmful for boundary localisation. TransUNet~\cite{chen2024transunet} introduced a hybrid CNN-transformer encoder for medical segmentation, establishing the value of combining local and global representations. In the medical domain, foundation models trained on large multimodal corpora have emerged as strong backbones. BiomedCLIP~\cite{zhang2025biomedclip} aligns biomedical image and text representations through contrastive pretraining on fifteen million scientific image-text pairs, producing semantically rich features that transfer well to segmentation. SAM~\cite{kirillov2023segment} and its medical adaptation SAM-Med2D~\cite{cheng2024sammed2d} offer strong interactive segmentation but lack inherent frequency-domain awareness. Our approach adopts BiomedCLIP as a frozen semantic backbone and complements it with an explicit frequency pathway, rather than relying on a single foundation model for all signal components.

\paragraph{Frequency-Domain Analysis in Deep Learning.}
Classical frequency analysis, including Fourier transforms, Laplacian operators, and wavelet decompositions, separates images into sub-bands that isolate different spatial scales and orientations~\cite{mallat1989wavelet}. In deep learning, frequency-domain methods have been applied to domain adaptation~\cite{yang2020FDA}, network robustness analysis, and medical segmentation. DUWS-Net~\cite{duwsnet2025} proposes a wavelet-based dual U-Net that fuses spatial and frequency representations through a spatial-frequency transformer. It shares architectural motivation with our dual-stream design but lacks language guidance. Huang and Zhou~\cite{huang2025frequency} address class imbalance through frequency re-weighting, treating frequency as an auxiliary signal rather than an independent encoding pathway. Our approach applies a learned encoder directly to multi-sub-band wavelet inputs, letting the network discover task-optimal frequency representations end-to-end while retaining the interpretability of the DWT decomposition.

\paragraph{Dual-Stream and Multi-Scale Fusion Architectures.}
Decomposing a task into complementary processing pathways has a long history in medical image analysis. nnU-Net~\cite{isensee2021nnunet} showed that careful single-stream U-Net training can match heavily engineered multi-stream systems, establishing a strong baseline. Attention U-Net~\cite{oktay2018attention} introduced gating mechanisms for selective skip-connection activation, anticipating the need for principled rather than unconditional feature fusion. In the transformer era, dual-pathway designs have been revisited: complementary pathways processing different resolutions or modalities exchange information through cross-attention rather than concatenation~\cite{li2024lvit}. The key insight is that bidirectional cross-attention at a shared bottleneck (the mechanism underlying our FFBI module) allows each pathway to selectively query the other for complementary information. This yields representations that neither stream could produce alone, which differs qualitatively from additive or concatenation fusion that lacks the selectivity afforded by attention.

\paragraph{Text-Guided Segmentation.}
CLIP~\cite{radford2021learning} established text-image alignment at scale, enabling language-conditioned vision systems. LViT~\cite{li2024lvit} integrates text into a U-Net-style architecture via cross-attention at the encoder, showing consistent improvements over visual-only baselines in medical segmentation. MedCLIP-SAM~\cite{koleilat2024medclip} and MedCLIP-SAMv2~\cite{koleilat2025medclipsamv2} bridge CLIP-based text features with SAM's prompt-driven segmentation, achieving strong zero-shot performance on diverse medical tasks. Li et al.~\cite{li2023unleashing} show that text prompts can resolve ambiguity in segmenting structures with overlapping appearance, an observation that motivates our use of language guidance at multiple decoder scales. Our LFFI modules apply FiLM conditioning~\cite{perez2018film} at each spatial resolution of both decoder branches, letting the text signal modulate boundary refinement progressively. This design has not been explored in prior text-guided segmentation work, where language conditioning is typically applied at a single scale or only at the encoder level.
