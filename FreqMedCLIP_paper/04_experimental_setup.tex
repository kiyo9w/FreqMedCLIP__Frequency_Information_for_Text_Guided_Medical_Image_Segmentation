\section{Dataset and Implementation Setup}
\label{sec:setup}

\paragraph{Datasets.}
We evaluate FreqMedCLIP on three medical imaging benchmarks spanning different anatomies and imaging modalities.
\textbf{Brain Tumor Segmentation (BraTS 2020)}~\cite{menze2015brats,bakas2017brats,mehta2022qu}: 369 subjects with high-grade glioma imaged with multimodal MRI (T1, T1c, T2, FLAIR). We extract 2D axial slices from 3D volumes and filter uninformative slices, yielding 8{,}000 training and 2{,}200 validation slices. Tumour sub-regions include necrotic core, peritumoral oedema, and enhancing tumour.
\textbf{Breast Cancer Segmentation (CBIS-DDSM)}~\cite{lee2017cbisddsm}: 1{,}566 full-field digital mammograms with binary lesion annotations. Region-of-interest crops ($512\times512$) are resized to $224\times224$, yielding 3{,}400 training and 800 validation examples.
\textbf{Lung Nodule Segmentation (LUNA16)}~\cite{setio2017luna16}: CT volumes from the LUNA16 challenge with 3D nodule masks annotated by four radiologists. We extract 2D axial slices to obtain 4{,}200 training and 1{,}000 validation examples from 1{,}018 volumes.
Table~\ref{tab:datasets} summarises the dataset statistics.

\begin{table}[h]
\centering
\caption{Dataset statistics. All splits follow a 70/20/10 stratified partition.}
\label{tab:datasets}
\begin{tabular}{@{}llrrrr@{}}
\toprule
Dataset & Modality & \#Train & \#Val & \#Test & Task \\
\midrule
BraTS 2020  & MRI (multi-modal) & 8{,}000  & 2{,}200  & 1{,}100 & Tumour segmentation \\
CBIS-DDSM   & Mammography (2D)  & 3{,}400  & 800    & 400   & Lesion segmentation \\
LUNA16      & CT (axial slices)  & 4{,}200  & 1{,}000  & 500   & Nodule segmentation \\
\bottomrule
\end{tabular}
\end{table}

Class imbalance is addressed through Dice loss and balanced sampling during training.
All images are resized to $224\times224$ and normalised per modality (z-score for MRI, min-max for CT and mammography).

\paragraph{Implementation Details.}
Table~\ref{tab:impl} lists the hyperparameters used for all experiments unless otherwise stated.

\begin{table}[h]
\centering
\caption{Implementation hyperparameters.}
\label{tab:impl}
\begin{tabular}{@{}ll@{}}
\toprule
Parameter & Value \\
\midrule
Framework            & PyTorch 2.0 \\
Hardware             & 4$\times$ NVIDIA A100 (40\,GB), DistributedDataParallel \\
Input resolution     & $224 \times 224 \times 3$ \\
Batch size           & 4 per GPU (16 total), gradient accumulation $\times$2 \\
Optimiser            & AdamW \\
Learning rate        & $1\times10^{-4}$ for trainable modules (BiomedCLIP backbone frozen) \\
LR scheduler         & CosineAnnealingLR ($T_{\max}=100$) \\
Maximum epochs       & 100 (typical convergence: 50--75) \\
Early stopping       & Patience 10 on validation Dice \\
Loss weights         & $\lambda=1.0$, $\alpha=\beta=0.4$ (deep supervision) \\
Inference            & ${\sim}80$\,ms per image (single A100), 3-checkpoint ensemble \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Evaluation Metrics.}
We report three complementary metrics. The \textbf{Dice coefficient}
$\mathrm{Dice} = {2|P \cap G|}/{(|P| + |G|)}$
serves as the primary overlap metric, following standard practice in medical segmentation~\cite{maier2024guideline}.
\textbf{Intersection-over-Union} (IoU) $= {|P \cap G|}/{|P \cup G|}$ provides a stricter overlap measure that penalises false positives more heavily.
The \textbf{Hausdorff Distance} (HD) measures worst-case boundary deviation in millimetres and serves as the primary boundary quality metric:
\begin{equation}
\mathrm{HD}(P,G) = \max\!\Big(\max_{p \in \partial P} \min_{g \in \partial G} d(p,g),\ \max_{g \in \partial G} \min_{p \in \partial P} d(p,g)\Big).
\end{equation}
