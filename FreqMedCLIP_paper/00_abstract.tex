\begin{abstract}
Medical image segmentation requires precise boundary detection while maintaining semantic understanding of anatomical structures. Vision transformers excel at capturing global semantic context but struggle with fine-grained boundary localization. Conversely, frequency-domain analysis captures edge details but lacks semantic awareness. We propose \textit{FreqMedCLIP}, a dual-stream architecture that harmonizes semantic and frequency information through language-guided feature fusion. The model combines a frozen BiomedCLIP vision transformer for semantic understanding with a learned frequency encoder that extracts high-frequency details from image Laplacian. A bidirectional fusion module (FFBI) enables symmetric information exchange between streams, while text-guided feature fusion (LFFI) at each decoder level provides semantic guidance for progressive refinement. Comprehensive experiments on brain tumor, breast tumor, and lung segmentation datasets demonstrate that the dual-stream design achieves superior Dice coefficient (0.87±0.02) compared to semantic-only baselines (0.81±0.03) and frequency-only approaches (0.73±0.05). Architectural ablations reveal that FFBI fusion contributes 3.2\% improvement, text guidance adds 2.1\% improvement, and frequency decomposition enhances boundary precision by 4.7\% over the semantic baseline. This work presents a principled approach to multi-stream medical image segmentation with explicit frequency-semantic interaction.

\keywords{FreqMedCLIP \and frequency-domain segmentation \and medical image segmentation \and cross-modal fusion \and prompt compliance}
\end{abstract}
