\begin{abstract}
Medical image segmentation requires both reliable boundary detection and semantic understanding of anatomical structures. Vision transformers capture global context well but suppress high-frequency image components through attention smoothing, producing blurred boundaries. Frequency-domain methods preserve edge detail but lack the semantic awareness to tell meaningful boundaries from noise. We propose \textit{FreqMedCLIP}, a dual-stream architecture that couples a frozen BiomedCLIP vision transformer with a learned frequency encoder operating on Haar wavelet sub-bands. A bidirectional cross-attention module (FFBI) enables symmetric information exchange between streams at a shared bottleneck, while language-guided FiLM conditioning (LFFI) provides text-driven semantic guidance at each decoder resolution. Experiments on brain tumor, breast lesion, and lung nodule segmentation benchmarks show that FreqMedCLIP achieves 0.867 average Dice coefficient (vs.\ 0.845 for SAM-Med2D and 0.805 for the semantic-only baseline) and 2.1\,mm Hausdorff distance (vs.\ 6.1\,mm for SAM-Med2D). Ablations confirm that the frequency encoder contributes $+4.2\%$ Dice, FFBI fusion adds $+3.0\%$, and LFFI text guidance adds $+2.1\%$.

\keywords{medical image segmentation \and frequency-domain analysis \and dual-stream architecture \and text-guided segmentation \and cross-modal fusion}
\end{abstract}
